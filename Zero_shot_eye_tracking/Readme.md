## A simple Panoptic segmentation of the pupil for eye tracking 

Using CLIP-Seg and combining Segment Anything, we can get a high-quality pupil segmentation, which can be used to track its trajectory using centroids. 

## Implementation details

![image](https://github.com/Elsword016/DataScience-and-ML-projects/assets/29883365/ee063ed5-3ffe-4921-b0e2-3d7c1e2ea162)

First, the frames are passed into the **CLIP-Seg** model and the pupil is identified with the text prompt **"pupil"**. The mask generated by the model is not of high quality, here's where the Segment Anything (SAM) model comes in. SAM is a promptable foundation model for segmentation. We can prompt the model with points, and bounding boxes to define the region for segmentation. So I sampled some points from the low-quality CLIP-Sam mask and prompted SAM with the points to generate the high-quality pupil mask.

## Results

- #### Segmentation

  The K-means method can be found [here](Eye_tracking), which I used for a competition, for isolating the pupil and generating segmentation and tracking as fast as possible without using any sophisticated methods. The lower panel shows how precise the mask obtained with Zero-shot panoptic segmentation is by combining two SOTA models.

![image](https://github.com/Elsword016/DataScience-and-ML-projects/assets/29883365/e34744ae-73b0-49c3-842b-cfb905523b98)

- #### Tracking
  + **Path length**

  I calculated the cumulative distance traveled as function of time (number of frames). The final value of the cumulative distance at the end of the plot represents the total distance traveled by the object throughout the entire sequence of frames.

  ![image](https://github.com/Elsword016/DataScience-and-ML-projects/assets/29883365/6407d995-60bd-4a85-8047-3efcb9ef530c)

  + **Trajectory**

    ![image](https://github.com/Elsword016/DataScience-and-ML-projects/assets/29883365/5a36ac36-e8bb-4275-8cab-c74f9a06224d)

## Models used
  - **CLIP-Seg**: Image Segmentation Using Text and Image Prompts. [[paper]](https://arxiv.org/abs/2112.10003) [[code]](https://github.com/timojl/clipseg)
  - **Segment-Anything**: Segment Anything Model (SAM) produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. [[paper]](https://ai.facebook.com/research/publications/segment-anything/) [[code]](https://github.com/facebookresearch/segment-anything)

## References
**CLIP-Seg**
  ```
@InProceedings{lueddecke22_cvpr,
    author    = {L\"uddecke, Timo and Ecker, Alexander},
    title     = {Image Segmentation Using Text and Image Prompts},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2022},
    pages     = {7086-7096}
}
```

**Segment Anything**
```
@article{kirillov2023segany,
  title={Segment Anything},
  author={Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C. and Lo, Wan-Yen and Doll{\'a}r, Piotr and Girshick, Ross},
  journal={arXiv:2304.02643},
  year={2023}
}
```
  




 
